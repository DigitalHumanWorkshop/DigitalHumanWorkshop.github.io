
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
  <!-- SITE TITTLE -->
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Home | Digital Human Workshop at AAAI 2024</title>
  
  <!-- PLUGINS CSS STYLE -->
  <link href="./res/jquery-ui.min.css" rel="stylesheet">
  <!-- Bootstrap -->
  <link href="./res/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link href="./res/font-awesome.min.css" rel="stylesheet">
  <!-- Owl Carousel -->
  <link href="./res/slick.css" rel="stylesheet">
  <link href="./res/slick-theme.css" rel="stylesheet">
  <!-- Fancy Box -->
  <link href="./res/jquery.fancybox.pack.css" rel="stylesheet">
  <link href="./res/nice-select.css" rel="stylesheet">
  <link href="./res/bootstrap-slider.min.css" rel="stylesheet">
  <!-- CUSTOM CSS -->
  <link href="./res/style.css" rel="stylesheet">
  
  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
 <style type="text/css">
  .left,
  .right {
    float: left;
    width: 50%;
    padding-left: 0px;
  }
  .left { padding-left: 20px; }
</style> 
  </head>
  
  <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="">
      <section>
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://aaai.org/aaai-conference/"> <img src="./asset/AAAI.jpeg" alt="" width="85px"> </a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                  <ul class="navbar-nav ml-auto main-nav">
                    <li class="nav-item active"> <a class="nav-link" href="#overview">Home</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#call">Calls</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#dates">Schedule</a></li>
                    <li class="nav-item"> <a class="nav-link " href="#challenge">Challenge</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#organizers">Organizer</a> </li>
                  </ul>
                  
                </div>
              </nav>
            </div>
          </div>
        </div>
      </section>
  
  
  <!--==========================================
  =            Overview Section            =
  ===========================================-->
  
  <section class="popular-deals section bg-gray"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-md-12"> 
          <!-- Section title -->
          <div class="section-title">
            <h1>1st International Workshop on <br>AI for Digital Human Modeling</h1>
      <br>
      <h4>Workshop at AAAI Conference on Artificial Intelligence 2024</h4>
      <br><br>
            <h2 id="overivew">Overview</h2>
          </div>
            It is a natural desire for human beings to investigate the dig-
          ital world, e.g., the metaverse. Digital human avatars, as the
          most common representation of human beings in the digi-
          tal space, are a fundamental element of the metaverse. Ac-
          cordingly, there is a growing interest in developing AI tools
          to facilitate the process and improve the quality of digital
          human creation. This workshop aims to bring together re-
          searchers interested in the latest advancements in the field
          of digital human modeling and how artificial intelligence can
          be leveraged to improve the quality and efficiency of the pro-
          cess. To create high-fidelity digital humans, traditional mod-
          els primarily follow the computer graphics pipeline. This
          process includes model creation, texture mapping, motion
          capture, bone binding, and other intricate procedures that re-
          quire heavy manual labor. As a result, it is infeasible to scale
          this process to large-scale digital human generation and this
          limits the applications of digital humans. Recent advances in
          artificial intelligence motivate us to propose this workshop,
          to further pursue a more automatic pipeline of modeling dig-
          ital humans.
      </div>
      </div>
    </div>
  </section>
  

    <!--==========================================
  =            Call for Papers Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="call">Call for Papers</h2>
          </div>

            This workshop will primarily focus on three main components in digital human modeling, i.e. representation, rendering, and learning.
           We encourage the submissions to address the opening questions such as: <br>
    <ul style="margin-left: 40px">
    <li>Whether explicit(mesh,point cloud, etc.) or implicit(SDF, NeRF, etc.) representations are more suitable for modeling digital humans</li>
    <li>What's the limitation of each representation and can we build a hybrid model?</li>
    <li>The traditional rendering pipeline has been well supported by industrial rendering engines, can AI tools such as neural rendering be integrated into the industry?</li>
    <li>There have been some pioneering works to apply 3D-aware GAN and diffusion models to generate digital humans, how to design new learning strategies that are more effective, efficient, and resource-friendly?</li>
    </ul>
      This workshop will also accept submissions for the downstream tasks of digital humans, including but not limited to generation, reconstruction, and animation.
    It is encouraged to submit the AI-empowered approaches to significantly reduce manual labor for digital human modeling, such as:
    <ul style="margin-left: 40px">
      <li>Large-scale learning for face/body generation</li>
      <li>Semi-supervised/unsupervised learning for parameterized digital human modeling</li>
      <li>Neural networks employed for generating digital humans with detailed geometry and textures</li>
      <li>Animating digital humans to precise expression, including talking head and motion control</li>
    </ul>
      Finally, this workshop will welcome submissions to discuss the impact of AI-generated characters on social good. 
    For example, the potential risk of creating fake media, invading people's privacy, replacing human workers in certain industries, etc.<br><br>
        <strong>Submission Format</strong>: Submissions papers (.pdf format) must use the AAAI Article Template and be anonymized and follow AAAI 2024 author instructions. The workshop considers two types of submissions: (1) Long Paper: Papers are limited to 7 pages excluding references; (2) Extended Abstract: Papers are limited to 4 pages including references.<br>
        <strong>Submission Site</strong>: <a href="https://cmt3.research.microsoft.com/PracticalDL2023/" style="color:blue;">https://cmt3.research.microsoft.com/PracticalDL2023/</a> 
        <br>
        <strong>Submission Due</strong>: 15th Nov, 2022 (AoE) 

        </div>
      </div>
    </div>
  </section>


  <!--===========================================
  =            Workshop Arrangement            =
  ============================================-->
  
  <section class="section">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="section-title">
            <h2 id="dates">Workshop Schedule</h2>
          </div>
            <!-- <p align="center">
                <img src="./res/schedule2023.png" width="90%">
            </p> -->
        </div>
      </div>
    </div>
  </section>


    <!--==========================================
  =            Challenge Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="challenge">Digital Human Challenge</h2>
          </div>
          <!-- Deep learning has achieved significant success in multimedia fields, however research in adversarial learning also shows that it is highly vulnerable to adversarial examples. We invite submissions on any aspect of adversarial machine learning in multimedia deep learning systems. -->
          The challenge is held jointly with the "1st International Workshop on High-fidelity Digital Human Reconstruction and Animation" at NeurIPS 2023.<br>
          <strong>Task 1: Self-Supervised Face Appearance Reconstruction</strong>
          <ul style="margin-left: 40px">
          <li>The objective of this task is to introduce self-supervised learning for face appearance reconstruction.
            Traditional photometric-based methods require capturing multi-view images under different lighting
            conditions to obtain high-quality facial appearance assets. However, this often requires the use of
            large devices like light-stages. In this competition, we encourage participants to calculate detailed
            digital face appearances from limited lighting and a small number of photos without relying on
            high-end equipment. Participants need to design a self-supervised learning framework to decompose
            detailed normal maps and a displacement map.</li>
          </ul>
          <strong>Task 2: Semi-supervised 3D Skull Reconstruction</strong>
          <ul style="margin-left: 40px">
          <li>Efficient, accurate, and robust algorithms for 3D skull reconstruction can play a significant role in
            achieving internal and external consistency in digital humans’ parametric models (see Figure 2).
            In this task, we expect the cranial reconstruction algorithm to reconstruct the anatomical geometry
            of the skull based on the information provided by MRI, including but not limited to the precise
            geometry of the external surface of the face, the internal soft tissue structures of the face, and
            the characteristic points of the skull. This competition encourages participants to develop a semi-
            supervised learning-based cranial segmentation algorithm for MRI images using machine learning and
            deep learning techniques. The segmentation algorithm can fully exploit the information of unlabeled
            training samples based on a small number of labeled samples while improving the performance and
            generalization of the segmentation model. Ultimately, the high-quality segmentation results obtained
            on MRI will serve as the basis for 3D skull reconstruction.</li>
          </ul>
          <strong>Task 3: Multi-modal Learning for Audio-driven Talking Head Generation</strong>
          <ul style="margin-left: 40px">
          <li>In this task, we specifically focus on the ability to generate talking heads with realistic facial
            expressions and natural head poses that match the accompanying audio. By learning from both the
            audio and visual data, this task focuses on developing a multi-modal learning model for talking head
            generation. We further design the following two settings to motivate the participants to tackle this
            task with 2D and 3D methods, respectively.
            Single image setting: Participants are allowed to train their model with external data, and we will
            provide a single image for their models to animate.
            Video setting: Participants are not allowed to train their model with external data, while a two-minute
            training video is provided for training a personalized talking head model.
            The final output of both settings should be a talking head model that can be driven by any input audio.
            For the image setting, the synthesized talking head videos are expected to be lip-synchronized and
            with high fidelity. As for the video setting, the speakers should additionally have various natural
            poses during talking while preserving their identity.</li>
          </ul>
          <strong>Task 4: Multi-modal Learning for Audio-driven Co-speech Gesture Synthesis</strong>
          <ul style="margin-left: 40px">
          <li>Nonverbal behaviors, especially co-speech gestures, are vital for face-to-face communication. Studies
            have shown that gestures can help express intentions, convey emotions, and complement the informa-
            tion conveyed by speech. Recent research has demonstrated that machine learning-based approaches
            play an increasingly critical role in motion synthesis, which has become the primary method for
            co-speech gesture synthesis.
            In this task, individuals and teams are required to synthesize 3D talking gestures based on any given
            audio. The final output should be a 3D motion sequence that can drive a given 3D avatar. Generally,
            the synthesized gesture motions in this task are expected to be natural, difficult to distinguish from
            captured human gesture motions, and consistent with audio in terms of rhythm, semantics, and style.
            We encourage participants to propose novel ideas for synthesizing high-fidelity co-speech gestures.</li>
          </ul>

          <strong>Challenge Site</strong>: <a href="https://practical-dl.sensecore.cn/" style="color:blue;">https://practical-dl.sensecore.cn</a> 
          <br>
          <strong>Submission Due</strong>: 31th Dec, 2022 (AoE)
        </div>
      </div>
    </div>
  </section>

<!--==========================================
=            Organizers Section            =
===========================================-->

<section class="popular-deals section bg-gray"> 
  <!-- Container Start -->
  <div class="container">
    <div class="row">
      <div class="col-12"> 
        <!-- Section title -->
        <div class="section-title">
          <h2 id="organizers">Organizer</h2>
        </div>

        <div class="card-deck"> 

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/yanyichao.jpeg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://daodaofr.github.io/">Yichao Yan</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Shanghai Jiaotong University<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/matthias.jpeg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://niessnerlab.org/">Matthias Nießner</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Technical University of Munich
                      <br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/dengjiankang.png" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://jiankangdeng.github.io/">Jiankang Deng</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Noah's Ark Lab<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/fengyao.jpeg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://is.mpg.de/~yfeng">Yao Feng</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>MPI for Intelligent Systems and ETH Zurich<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
                            
  </div>
  <br>
  
        <div class="card-deck"> 

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/szafeiriou.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p>Imperial College London<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
                   
          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/daizonghong.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://ieeexplore.ieee.org/author/37088356489">Zonghong Dai</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/xudi.gif" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://ieeexplore.ieee.org/author/37086190386">Di Xu</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href=""></a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
  </div>

      </div>
    </div>
  </div>
  <!-- Container End --> 
  </section>



    <!--==========================================
  =            Paper Submission Section            =
  ===========================================-->
  
  <!--  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="section-title">
            <h2 id="paper-submission">Paper Submission</h2>
          </div>
          <strong>Format</strong>: Submissions need to be anonymized and follow AAAI 2022 author instructions. The workshop considers two types of submissions: (1) Long Paper: Papers are limited to <strong>7</strong> pages excluding references; (2) Extended Abstract: Papers are limited to <strong>4</strong> pages including references.

    <br>
    <strong>Submission Server</strong>: <a href="https://cmt3.research.microsoft.com/">https://cmt3.research.microsoft.com/</a> .
    
  </div>
  </div>
  
-->
    <!-- br>
    <br> -->
    
  <!--============================
  =            Footer            =
  =============================-->
  
  <!-- Footer Bottom -->
  <footer class="footer-bottom"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12 col-12"> 
          <!-- Copyright -->
          <div class="copyright">
            <p class="text-center">For any further questions, you can contact <a href="mailto:qinhaotong@buaa.edu.cn"><font color="#5672f9">Haotong Qin</font></a>, <a href="mailto:gongruihao@sensetime.com"><font color="#5672f9">Ruihao Gong</font></a>, and <a href="mailto:liuaishan@buaa.edu.cn"><font color="#5672f9">Aishan Liu</font></a>.</p>
            <!-- <p class="text-center">For any further questions, you can contact <a href="mailto:qinhaotong@buaa.edu.cn"><font color="#5672f9">Haotong Qin</font></a>, <a href="mailto:xinyun.chen@berkeley.edu"><font color="#5672f9">Xinyun Chen</font></a>, or <a href="mailto:yingwei.li@jhu.edu"><font color="#5672f9">Yingwei Li</font></a>.</p> -->
              <br class="clear">
          </div>
        </div>
      </div>
    </div>
  </footer>
  
  <!-- JAVASCRIPTS --> 
  <script src="./res/jquery.min.js"></script> 
  <script src="./res/jquery-ui.min.js"></script> 
  <script src="./res/tether.min.js"></script> 
  <script src="./res/jquery.raty-fa.js"></script> 
  <script src="./res/popper.min.js"></script> 
  <script src="./res/bootstrap.min.js"></script> 
  <script src="./res/bootstrap-slider.min.js"></script> 
  <script src="./res/slick.min.js"></script> 
  <script src="./res/jquery.nice-select.min.js"></script> 
  <script src="./res/jquery.fancybox.pack.js"></script> 
  <script src="./res/SmoothScroll.min.js"></script> 
  <script src="./res/scripts.js"></script>
  
  
  
</body></html>
