
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
  <!-- SITE TITTLE -->
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Home | Digital Human Workshop at AAAI 2024</title>
  
  <!-- PLUGINS CSS STYLE -->
  <link href="./res/jquery-ui.min.css" rel="stylesheet">
  <!-- Bootstrap -->
  <link href="./res/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link href="./res/font-awesome.min.css" rel="stylesheet">
  <!-- Owl Carousel -->
  <link href="./res/slick.css" rel="stylesheet">
  <link href="./res/slick-theme.css" rel="stylesheet">
  <!-- Fancy Box -->
  <link href="./res/jquery.fancybox.pack.css" rel="stylesheet">
  <link href="./res/nice-select.css" rel="stylesheet">
  <link href="./res/bootstrap-slider.min.css" rel="stylesheet">
  <!-- CUSTOM CSS -->
  <link href="./res/style.css" rel="stylesheet">
  
  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
 <style type="text/css">
  .left,
  .right {
    float: left;
    width: 50%;
    padding-left: 0px;
  }
  .left { padding-left: 20px; }
</style> 
  </head>
  
  <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="">
      <section>
        <div class="container">
          <div class="row">
            <div class="col-md-7">
              <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://aaai.org/aaai-conference/"> <img src="./asset/AAAI.jpeg" alt="" width="85px"> </a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                  <ul class="navbar-nav ml-auto main-nav justify-content-center">
                    <li class="nav-item active"> <a class="nav-link" href="#overview">Home</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#call">Calls</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#dates">Schedule</a></li>
                    <li class="nav-item"> <a class="nav-link " href="#challenge">Challenge</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#organizers">Organizer</a> </li>
                  </ul>
                  
                </div>
              </nav>
            </div>
          </div>
        </div>
      </section>
  
  
  <!--==========================================
  =            Overview Section            =
  ===========================================-->
  
  <section class="popular-deals section bg-gray"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-md-12"> 
          <!-- Section title -->
          <div class="section-title">
            <h1>International Workshop on <br>AI for Digital Human</h1>
      <br>
      <h4>Workshop at AAAI Conference on Artificial Intelligence 2024</h4>
      <br><br>
            <h2 id="overivew">Overview</h2>
          </div>
          It is a natural desire for human beings to investigate the digital world, e.g., the metaverse.Digital human avatars, as the most common representation of human beings in the digital space, are a fundamental element of the metaverse.Accordingly, there is a growing interest in developing AI tools to facilitate the process and improve the quality of digital human creation. This workshop aims to bring together researchers interested in the latest advancements in the field of digital human and how artificial intelligence can be leveraged to improve the quality and efficiency of the process.
          <br>
          <br>
          This workshop covers a broad scope of digital humans, encouraging the theoretical contributions, the downstream applications, and the discussion of social impacts. This workshop gives an opportunity to systematically explore digital humans in a unified perspective from the field of artificial intelligence, and it opens to not only computer researchers but also experts focusing on law, education, psychology, sociology, etc. In this workshop, the latest developments in these fields can be put together, to inspire the interdisciplinary study of AI and digital humans.
      </div>
      </div>
    </div>
  </section>
  

    <!--==========================================
  =            Call for Papers Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="call">Call for Papers</h2>
          </div>

          Topics of interest include, but are not limited to, the following: <br>
    <ul style="margin-left: 40px">
    <li>AI models and algorithms for digital human modeling; explicit and implicit representations; AI-empowered rendering technique such as neural rendering; learning strategies that are more effective, efficient, and resource-friendly</li>
    <li>Downstream tasks of digital humans; large foundation models for digital human generation; machine learning for face, body, hair and clothing reconstruction; face/body animation with audio or texts</li>
    <li>The social impact of AI-generated characters. For example, the potential to transform industries including health and education, the potential risk of creating fake media, invading people’s privacy, impacting human workers in certain industries, etc.</li>
    <li>Other relevant applications and methods, e.g. digital human in VR and metaverse, etc.</li>
    </ul>
    This will be a one-day workshop. In the morning session, we will have three invited speakers and a panel to discuss the important challenges in the field of AI for digital humans. In the afternoon session, we will have an oral session for the authors of the submissions to share their works. If we receive many good submissions, we will also organize a poster session. Additionally, we will organize a competition together with this workshop, and the winners will be announced during the meeting. We expect 50 or so attendance and open the workshop to all AAAI-24 participants.
    <br>
    <br>
    <strong>Submission Format</strong>:
    <ul style="margin-left: 40px">
      <li>Technical Papers: Full-length research papers of up to 7 pages (excluding references and appendices)</li>
      <li>Short Papers: Position or short papers of up to 4 pages (excluding references and appendices)</li>
    </ul>
    All papers must be submitted in PDF format, using the AAAI-24 author kit. All submissions should be done electronically via CMT.
    <br>
    <strong>Submission Site</strong>: <a href="https://cmt3.research.microsoft.com/AI4DH2024" style="color:blue;">https://cmt3.research.microsoft.com/AI4DH2024</a> 
        <br>
        <strong>Submission Due</strong>: November 24, 2023 (11:59 PM PST) 

        </div>
      </div>
    </div>
  </section>


  <!--===========================================
  =            Workshop Arrangement            =
  ============================================-->
  
  <section class="section">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="section-title">
            <h2 id="dates">Workshop Schedule</h2>
          </div>
            <p align="center">
                <img src="./asset/schedule.png" width="90%">
            </p>
        </div>
      </div>
    </div>
  </section>


    <!--==========================================
  =            Challenge Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="challenge">Digital Human Challenge</h2>
          </div>
          <!-- Deep learning has achieved significant success in multimedia fields, however research in adversarial learning also shows that it is highly vulnerable to adversarial examples. We invite submissions on any aspect of adversarial machine learning in multimedia deep learning systems. -->
          The challenge will be released at <b>Nov 1st, 2023</b>.<br>
          <strong>Task 1: Self-Supervised Face Appearance Reconstruction</strong>
          <ul style="margin-left: 40px">
          <li>The objective of this task is to introduce self-supervised learning for face appearance reconstruction.
            Traditional photometric-based methods require capturing multi-view images under different lighting
            conditions to obtain high-quality facial appearance assets. However, this often requires the use of
            large devices like light-stages. In this competition, we encourage participants to calculate detailed
            digital face appearances from limited lighting and a small number of photos without relying on
            high-end equipment. Participants need to design a self-supervised learning framework to decompose
            detailed normal maps and a displacement map.</li>
          <li><b>Kaggle:</b>   <a href="https://www.kaggle.com/competitions/self-supervised-face-geometry-reconstruction" style="color:blue;">https://www.kaggle.com/competitions/self-supervised-face-geometry-reconstruction</a></li>
          <li><b>Organizers:</b>   Yuhao Cheng, Xingyu Ren, Xuanchen Li (Shanghai Jiao Tong University), Huang Xu (Huawei Cloud) </li>
          <li><b>Contact:</b>   chengyuhao@sjtu.edu.cn</li>
          </ul>
          <strong>Task 2: Semi-supervised 3D Skull Reconstruction</strong>
          <ul style="margin-left: 40px">
          <li>
            In this task, we expect the cranial reconstruction algorithm to reconstruct the anatomical geometry
            of the skull based on the information provided by MRI, including but not limited to the precise
            geometry of the external surface of the face, the internal soft tissue structures of the face, and
            the characteristic points of the skull. This competition encourages participants to develop a semi-
            supervised learning-based cranial segmentation algorithm for MRI images using machine learning and
            deep learning techniques. The segmentation algorithm can fully exploit the information of unlabeled
            training samples based on a small number of labeled samples while improving the performance and
            generalization of the segmentation model. Ultimately, the high-quality segmentation results obtained
            on MRI will serve as the basis for 3D skull reconstruction.</li>
          <li><b>Kaggle:</b>   <a href="https://www.kaggle.com/competitions/3D-skull-reconstruction" style="color:blue;">https://www.kaggle.com/competitions/3d-skull-reconstruction</a></li>
          <li><b>Organizers:</b>   Hengfei Cui, Yong Xia, Fan Zheng, Yifan Wang (Northwestern Polytechnical University) </li>
          <li><b>Contact:</b>   zhengfan@mail.nwpu.edu.cn</li>
          </ul>
          <strong>Task 3: Multi-modal Learning for Audio-driven Talking Head Generation</strong>
          <ul style="margin-left: 40px">
          <li>In this task, we specifically focus on the ability to generate talking heads with realistic facial
            expressions and natural head poses that match the accompanying audio. By learning from both the
            audio and visual data, this task focuses on developing a multi-modal learning model for talking head
            generation. We further design the following two settings to motivate the participants to tackle this
            task with 2D and 3D methods, respectively.<br>
            Single image setting: Participants are allowed to train their model with external data, and we will
            provide a single image for their models to animate.<br>
            Video setting: Participants are not allowed to train their model with external data, while a two-minute
            training video is provided for training a personalized talking head model.<br>
            The final output of both settings should be a talking head model that can be driven by any input audio.
            For the image setting, the synthesized talking head videos are expected to be lip-synchronized and
            with high fidelity. As for the video setting, the speakers should additionally have various natural
            poses during talking while preserving their identity.</li>
          <li><b>Kaggle:</b>   <a href="https://www.kaggle.com/competitions/audio-driven-talking-head-generation/" style="color:blue;">https://www.kaggle.com/competitions/audio-driven-talking-head-generation/</a></li>
          <li><b>Organizers:</b>   Jingnan Gao(Shanghai Jiao Tong University) , Changpeng Yang, Yuan Gao, Li Li (Huawei Cloud) </li>
          <li><b>Contact:</b>   gjn0310@sjtu.edu.cn</li>
          </ul>
          <strong>Task 4: Multi-modal Learning for Audio-driven Co-speech Gesture Synthesis</strong>
          <ul style="margin-left: 40px">
          <li>
            In this task, participants are required to synthesize co-speech gesture videos of a target person based on any given audio. 
            The final output should be a rendered video, instead of motion sequences. 
            Generally, the synthesized gesture motions in this task are expected to be natural, difficult to distinguish from captured videos, 
            and consistent with audio in terms of rhythm, semantics, and style. 
            We encourage participants to propose novel ideas for synthesizing high-fidelity co-speech gestures.</li>
          <li><b>Kaggle:</b>   <a href="https://www.kaggle.com/competitions/audio-driven-co-speech-gesture-video-generation" style="color:blue;">https://www.kaggle.com/competitions/audio-driven-co-speech-gesture-video-generation</a></li>
          <li><b>Organizers:</b>   Minglei Li, Haoqi Zeng(Huawei Cloud),Zhensong Zhang, Xiaofei Wu, Yiren Zhou (Huawei Noah’s Ark Lab) </li>
          <li><b>Contact:</b>   zenghaoqi@huawei.com</li>
          </ul>

          <!-- <strong>Challenge Site</strong>: <a href="https://practical-dl.sensecore.cn/" style="color:blue;">https://practical-dl.sensecore.cn</a> 
          <br>
          <strong>Submission Due</strong>: 31th Dec, 2022 (AoE) -->
        </div>
      </div>
    </div>
  </section>

<!--==========================================
=            Organizers Section            =
===========================================-->

<section class="popular-deals section bg-gray"> 
  <!-- Container Start -->
  <div class="container">
    <div class="row">
      <div class="col-12"> 
        <!-- Section title -->
        <div class="section-title">
          <h2 id="organizers">Organizer</h2>
        </div>

        <div class="card-deck"> 

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/yanyichao.jpeg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://daodaofr.github.io/">Yichao Yan</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Shanghai Jiao Tong University<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/dixu.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://ieeexplore.ieee.org/author/37086190386">Di Xu</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud Computing Technologies Co., Ltd<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/haozhejia.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://scholar.google.com.au/citations?user=8YMYwMwAAAAJ&hl=en9">Haozhe Jia</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud Computing Technologies Co., Ltd<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
  </div>
  <br>
    <div class="card-deck"> 
        <div class="card">
          <div class="category-block">
            <table width="80%" border="0" align="center">
              <tbody align="center">
                <tr height="200px">
                  <td><img src="./asset/matthias.jpeg" alt="" class="rounded-circle" width="60%"></td>
                </tr>
                <tr height="30px">
                  <td><h5 class="text-center"><a href="https://niessnerlab.org/">Matthias Nießner</a></h5></td>
                </tr>
                <tr height="60px">
                  <td><p><br>Technical University of Munich
                    <br><br><br><br></p></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

        <div class="card">
          <div class="category-block">
            <table width="80%" border="0" align="center">
              <tbody align="center">
                <tr height="200px">
                  <td><img src="./asset/dengjiankang.png" alt="" class="rounded-circle" width="60%"></td>
                </tr>
                <tr height="30px">
                  <td><h5 class="text-center"><a href="https://jiankangdeng.github.io/">Jiankang Deng</a></h5></td>
                </tr>
                <tr height="60px">
                  <td><p><br> Imperial College London，The department of
                    computing<br><br><br></p></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/kangkangyin.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://www.cs.sfu.ca/~kkyin/">KangKang Yin</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p>Simon Fraser University, School of Computing Science<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
                   
  </div>

      </div>
    </div>
  </div>
  <!-- Container End --> 
  </section>



    <!--==========================================
  =            Paper Submission Section            =
  ===========================================-->
  
  <!--  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="section-title">
            <h2 id="paper-submission">Paper Submission</h2>
          </div>
          <strong>Format</strong>: Submissions need to be anonymized and follow AAAI 2022 author instructions. The workshop considers two types of submissions: (1) Long Paper: Papers are limited to <strong>7</strong> pages excluding references; (2) Extended Abstract: Papers are limited to <strong>4</strong> pages including references.

    <br>
    <strong>Submission Server</strong>: <a href="https://cmt3.research.microsoft.com/">https://cmt3.research.microsoft.com/</a> .
    
  </div>
  </div>
  
-->
    <!-- br>
    <br> -->
    
  <!--============================
  =            Footer            =
  =============================-->
  
  <!-- Footer Bottom -->
  <footer class="footer-bottom"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12 col-12"> 
          <!-- Copyright -->
          <div class="copyright">
            <p class="text-center">For any further questions, you can contact <a href="mailto:yanyichao@sjtu.edu.cn"><font color="#5672f9">Yichao Yan</font></a>.</p>
            <!-- <p class="text-center">For any further questions, you can contact <a href="mailto:qinhaotong@buaa.edu.cn"><font color="#5672f9">Haotong Qin</font></a>, <a href="mailto:xinyun.chen@berkeley.edu"><font color="#5672f9">Xinyun Chen</font></a>, or <a href="mailto:yingwei.li@jhu.edu"><font color="#5672f9">Yingwei Li</font></a>.</p> -->
              <br class="clear">
          </div>
        </div>
      </div>
    </div>
  </footer>
  
  <!-- JAVASCRIPTS --> 
  <script src="./res/jquery.min.js"></script> 
  <script src="./res/jquery-ui.min.js"></script> 
  <script src="./res/tether.min.js"></script> 
  <script src="./res/jquery.raty-fa.js"></script> 
  <script src="./res/popper.min.js"></script> 
  <script src="./res/bootstrap.min.js"></script> 
  <script src="./res/bootstrap-slider.min.js"></script> 
  <script src="./res/slick.min.js"></script> 
  <script src="./res/jquery.nice-select.min.js"></script> 
  <script src="./res/jquery.fancybox.pack.js"></script> 
  <script src="./res/SmoothScroll.min.js"></script> 
  <script src="./res/scripts.js"></script>
  
  
  
</body></html>
