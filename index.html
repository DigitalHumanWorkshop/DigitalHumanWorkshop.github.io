
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
  <!-- SITE TITTLE -->
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Home | Digital Human Workshop at AAAI 2024</title>
  
  <!-- PLUGINS CSS STYLE -->
  <link href="./res/jquery-ui.min.css" rel="stylesheet">
  <!-- Bootstrap -->
  <link href="./res/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link href="./res/font-awesome.min.css" rel="stylesheet">
  <!-- Owl Carousel -->
  <link href="./res/slick.css" rel="stylesheet">
  <link href="./res/slick-theme.css" rel="stylesheet">
  <!-- Fancy Box -->
  <link href="./res/jquery.fancybox.pack.css" rel="stylesheet">
  <link href="./res/nice-select.css" rel="stylesheet">
  <link href="./res/bootstrap-slider.min.css" rel="stylesheet">
  <!-- CUSTOM CSS -->
  <link href="./res/style.css" rel="stylesheet">
  
  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
 <style type="text/css">
  .left,
  .right {
    float: left;
    width: 50%;
    padding-left: 0px;
  }
  .left { padding-left: 20px; }
</style> 
  </head>
  
  <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="">
      <section>
        <div class="container">
          <div class="row">
            <div class="col-md-7">
              <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://aaai.org/aaai-conference/"> <img src="./asset/AAAI.jpeg" alt="" width="85px"> </a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                  <ul class="navbar-nav ml-auto main-nav justify-content-center">
                    <li class="nav-item active"> <a class="nav-link" href="#overview">Home</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#call">Calls</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#dates">Schedule</a></li>
                    <li class="nav-item"> <a class="nav-link " href="#challenge">Challenge</a> </li>
                    <li class="nav-item"> <a class="nav-link " href="#organizers">Organizer</a> </li>
                  </ul>
                  
                </div>
              </nav>
            </div>
          </div>
        </div>
      </section>
  
  
  <!--==========================================
  =            Overview Section            =
  ===========================================-->
  
  <section class="popular-deals section bg-gray"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-md-12"> 
          <!-- Section title -->
          <div class="section-title">
            <h1>International Workshop on <br>AI for Digital Human</h1>
      <br>
      <h4>Workshop at AAAI Conference on Artificial Intelligence 2024</h4>
      <br><br>
            <h2 id="overivew">Overview</h2>
          </div>
          It is a natural desire for human beings to investigate the digital world, e.g., the metaverse.Digital human avatars, as the most common representation of human beings in the digital space, are a fundamental element of the metaverse.Accordingly, there is a growing interest in developing AI tools to facilitate the process and improve the quality of digital human creation. This workshop aims to bring together researchers interested in the latest advancements in the field of digital human and how artificial intelligence can be leveraged to improve the quality and efficiency of the process.
          <br>
          <br>
          This workshop covers a broad scope of digital humans, encouraging the theoretical contributions, the downstream applications, and the discussion of social impacts. This workshop gives an opportunity to systematically explore digital humans in a unified perspective from the field of artificial intelligence, and it opens to not only computer researchers but also experts focusing on law, education, psychology, sociology, etc. In this workshop, the latest developments in these fields can be put together, to inspire the interdisciplinary study of AI and digital humans.
      </div>
      </div>
    </div>
  </section>
  

    <!--==========================================
  =            Call for Papers Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="call">Call for Papers</h2>
          </div>

          Topics of interest include, but are not limited to, the following: <br>
    <ul style="margin-left: 40px">
    <li>AI models and algorithms for digital human modeling; explicit and implicit representations; AI-empowered rendering technique such as neural rendering; learning strategies that are more effective, efficient, and resource-friendly</li>
    <li>Downstream tasks of digital humans; large foundation models for digital human generation; machine learning for face, body, hair and clothing reconstruction; face/body animation with audio or texts</li>
    <li>The social impact of AI-generated characters. For example, the potential to transform industries including health and education, the potential risk of creating fake media, invading people’s privacy, impacting human workers in certain industries, etc.</li>
    <li>Other relevant applications and methods, e.g. digital human in VR and metaverse, etc.</li>
    </ul>
    This will be a one-day workshop. In the morning session, we will have three invited speakers and a panel to discuss the important challenges in the field of AI for digital humans. In the afternoon session, we will have an oral session for the authors of the submissions to share their works. If we receive many good submissions, we will also organize a poster session. Additionally, we will organize a competition together with this workshop, and the winners will be announced during the meeting. We expect 50 or so attendance and open the workshop to all AAAI-24 participants.
    <br>
    <br>
    <strong>Submission Format</strong>:
    <ul style="margin-left: 40px">
      <li>Technical Papers: Full-length research papers of up to 7 pages (excluding references and appendices)</li>
      <li>Short Papers: Position or short papers of up to 4 pages (excluding references and appendices)</li>
    </ul>
    All papers must be submitted in PDF format, using the AAAI-24 author kit. All submissions should be done electronically via CMT.
    <br>
    <strong>Submission Site</strong>: <a href="https://cmt3.research.microsoft.com/AI4DH2024" style="color:blue;">https://cmt3.research.microsoft.com/AI4DH2024</a> 
        <br>
        <strong>Submission Due</strong>: November 24th, 2023 (11:59 PM PST) 

        </div>
      </div>
    </div>
  </section>


  <!--===========================================
  =            Workshop Arrangement            =
  ============================================-->
  
<style>

.pdf-link {
    color: blue; /* 设置链接颜色 */
    text-decoration: none; /* 去除下划线 */
    /* 其他样式 */
}

.schedule-table {
    width: 100%; /* Adjust the width as needed */
    margin: 0 auto; /* Center the table on the page */
    border-collapse: collapse;
    text-align: center; /* Center text in cells */
}

.schedule-table th, .schedule-table td {
    border: 1px solid black; /* Add borders to cells */
    padding: 10px; /* Add some padding for aesthetics */
}

.schedule-table th:nth-of-type(1), .schedule-table td:nth-of-type(1) {
  width: 18%; /* Width of the first column */
}

.schedule-table th:nth-of-type(2), .schedule-table td:nth-of-type(2) {
  width: 57%; /* Width of the second column */
}

.schedule-table th:nth-of-type(3), .schedule-table td:nth-of-type(3) {
  width: 25%; /* Width of the third column */
}
.schedule-table tr:first-child {
  background-color: rgb(175, 173, 173); /* Set your desired shade of gray */
}
.gray_background {
  background-color: rgb(235, 235, 235); /* Set your desired shade of gray */
}

.rank-table{
  width: 100%; /* Adjust the width as needed */
  margin: 0 auto; /* Center the table on the page */
  border-collapse: collapse;
  text-align: center; /* Center text in cells */
}
.rank-table th, .rank-table td {
  border: 1px solid black; /* Add borders to cells */
  padding: 10px; /* Add some padding for aesthetics */
}
.rank-table th:nth-of-type(1), .rank-table td:nth-of-type(1) {
  width: 18%; /* Width of the first column */
}
.rank-table th:nth-of-type(2), .rank-table td:nth-of-type(2) {
  width: 57%; /* Width of the second column */
}
.rank-table th:nth-of-type(3), .rank-table td:nth-of-type(3) {
  width: 25%; /* Width of the third column */
}
.rank-table tr:first-child {
  background-color: rgb(175, 173, 173); /* Set your desired shade of gray */
}

.bubble {
        position: absolute;
        background-color: #f9f9f9;
        border: 1px solid #ccc;
        border-radius: 5px;
        padding: 10px;
        display: none;
    }

.bubble:after {
        content: '';
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -5px;
        border-width: 5px;
        border-style: solid;
        border-color: #ccc transparent transparent transparent;
    }

.show {
        display: block;
}

#toggleButton {
        margin-top: 20px;
}

.highlight1 {
  color:red;
  font-weight: bold;
  font-size: 20px;
  text-align: center;
}

.highlight2 {
  color: black;
  font-weight: bold;
  font-style: italic;
  font-size: 16px;
  text-align: center;
}

</style>

  <section class="section">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <div class="section-title">
            <h2 id="dates">Workshop Schedule</h2>
          </div>
            <p><span class="highlight1">Best Paper Award:</span><span class="highlight2">Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation</span></p>
            <table class="schedule-table">
              <tr>
                <th>Time</th>
                <th>Arrangement</th>
                <th>Details</th>
              </tr>
              <tr>
                <td>9:00 AM-9:45 AM</td>
                <td><b>Keynote Speech:</b><br> Digital Humans: Science and Simulation in the Time of AI<br>
                <button id="toggleButton">Abstract</button>
                <div id="bubbleContent" class="bubble">My goal is to develop realistic computational models of the human body. Models that can make useful predictions of how we move and how our bodies interact with products such as clothing. Despite a long history of scientific research, current models have many surprising shortcomings.  I will first describe some fundamental problems with classical models of the complex human biomechanical system, and show how we can model it better. I will present examples of modeling human hands, eyes, muscles, and skin. I will describe breakthroughs in my lab in building personalized models of an individual's body using a complete pipeline for measurement, modeling, parameter estimation, and data-driven simulation using the finite element method. Our methods can be used to create personalized digital avatars of individuals or of a population. There are many potential applications, ranging from virtual prototyping for product design to virtual garment try-on for e-commerce. Next I will describe challenges and opportunities for machine learning in simulating the behavior of living humans. I will argue that access to high quality data remains the crucial bottleneck for these problems. How can we acquire such data, ethically and at scale?  I will also explore some fundamental technical limits to sensing and accurately predicting the actual behaviors of a real system, and not just plausible behaviors.</div>
                <script>
                    document.getElementById("toggleButton").addEventListener("click", function() {
                        var bubbleContent = document.getElementById("bubbleContent");
                        bubbleContent.classList.toggle("show");
                    });
                </script>
                </td>
                <td><b>Invited Speakers:</b><br> Dinesh K. Pai, University of British Columbia <br>
                <button id="toggleButton2">Bio</button>
                <div id="bubbleContent2" class="bubble">Dinesh K. Pai is a Professor of Computer Science at the University of British Columbia, and founder of Vital Mechanics Research - a startup providing high-fidelity soft avatars for apparel fit testing. His current research is focused on data-driven digital human models, frictional contact between soft objects, machine learning for design, and technology for efficient measurement of material properties. His research is multidisciplinary, spanning computer graphics, scientific computing, robotics, biomechanics, neuroscience, and artificial intelligence.
                  He has received many prestigious recognitions, including a Tier 1 Canada Research Chair, the 2020 CHCCS Achievement Award for Computer Graphics, UBC's Killam Research Prize, three NSERC Discovery Accelerator/Supplement Awards, and an international Human Frontier Science Program grant. Dr. Pai has been a Professor at Rutgers University and has held visiting professorships at Carnegie Mellon University's Robotics Institute, New York University's Center for Neural Science, the University of Siena (Santa Chiara Chair in Cognitive Science), and the Collège de France, Paris (Professeur Invité). He received his Ph.D. from Cornell University, Ithaca, NY, and his B.Tech. degree from the Indian Institute of Technology, Madras. See <a href="//sensorimotor.cs.ubc.ca/pai/ " style="color:blue;">//sensorimotor.cs.ubc.ca/pai/ </a>for more information.</div>
                  <script>
                    document.getElementById("toggleButton2").addEventListener("click", function() {
                        var bubbleContent = document.getElementById("bubbleContent2");
                        bubbleContent.classList.toggle("show");
                    });
                </script>
                </td>
              </tr>
              <tr>
                <td>9:45 AM-10:30 AM</td>
                <td><b>Invited Talk:</b><br> Deep Albedo: Real-time biophysically-based facial map modifcations<br>
                <button id="toggleButton3">Abstract</button>
                <div id="bubbleContent3" class="bubble">Altering skin parameters and colors are inherently complex and requires sophisticated physically-based models to describe. We leverage the autoencoders and simulated biophysically-based skin parameters to enable an efficient spatial-varying mapping between the biophysical parameters and their resulting skin color. The mapping enables the pixel-wise to describe age and emotion related skin color variations.</div>
                <script>
                  document.getElementById("toggleButton3").addEventListener("click", function() {
                      var bubbleContent = document.getElementById("bubbleContent3");
                      bubbleContent.classList.toggle("show");
                  });
              </script>  
              </td>
                <td><b>Invited Speakers:</b><br> Wei Sen Loi, Joel Johnson, Huawei Technologies Canada </td>
              </tr>
              <tr>
                <td>10:30 AM-11:00 AM</td>
                <td><b>Break</b></td>
                <td></td>
              </tr>
              <tr class="gray_background">
                <td colspan="3"><b>11:00 AM-12:30 PM——Oral Session 1</b></td>
              </tr>
              <tr>
                <td>11:00 AM-11:20 AM</td>
                <td>FuRPE: Learning Full-body Reconstruction from Part Experts <br>
                  <b>Authors</b>: Zhaoxin Fan, Yuqing Pan, Hao Xu, Zhenbo Song Zhicheng Wang, Kejian</td>
                <td>
                  <a href="./asset/CameraReady/FuRPE.pdf" class="pdf-link" target="_blank">[PDF]</a>
                </td>
              </tr>
              <tr>
                <td>11:20 AM-11:40 AM</td>
                <td>ProbSIP: Probabilistic Modeling for Ambiguity-Reduced Sparse Inertial Poser <br>
                <b>Authors</b>:Shanyan Guan, Yunbo Wang, Xintao Lv, Yanhao Ge, Xiaokang Yang</td>
                <td>
                  <a href="./asset/CameraReady/ProSIP.pdf" class="pdf-link" target="_blank">[PDF]</a>
                </td>
              </tr>
              <tr>
                <td>11:40 AM-12:00 PM</td>
                <td>Deep Learning based Dialogue System for Legal Consultancy in Smart Law<br>
                <b>Authors</b>:Xukang Wang, Ying Cheng Wu, Xuhesheng Chen, Hongpeng Fu, Jiaqi Tan, Mengjie Zhou</td>
                <td><a href="./asset/CameraReady/Deep Learning based Dialogue System for Legal.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>12:00 PM-12:20 PM</td>
                <td> Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation<br>
                <b>Authors</b>: Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu</td>
                <td><a href="./asset/CameraReady/Block-wise LoRA Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>12:20 PM-12:30 PM</td>
                <td>Announcement of Competition Awards</td>
                <td></td>
              </tr>
              <tr class="gray_background">
                <td colspan="3"><b>12:30 PM-2:00 PM——Lunch</b></td>
              </tr>
              <tr class="gray_background">
                <td colspan="3"><b>2:00 PM-4:00 PM——Oral Session 2</b></td>
              </tr>
              <tr>
                <td>2:00 PM-2:20 PM</td>
                <td>Structural Learning in the design of Perspective-Aware AI Systems using Knowledge Graphs<br>
                <b>Authors</b>: Marjan Alirezaie, Hossein Rahnama, Alex Pentland</td>
                <td><a href="./asset/CameraReady/Structure Learning.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>2:20 PM-2:40 PM</td>
                <td>The Role of Facial and Speech Features in Emotion Classification<br>
                <b>Authors</b>:Loïc Houmard, Ard Kastrati, Dushan Vasilevski, Roger Wattenhofer</td>
                <td><a href="./asset/CameraReady/The Role of.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>2:40 PM-3:00 PM</td>
                <td>Understanding Consumers' Attitude Toward Digital Humans In Influencer Marketing<br>
                <b>Authors:</b>Smitha Muthya Sudheendra, Maral Abdollahi, Jisu Huh, Jaideep Srivastava</td>
                <td><a href="./asset/CameraReady/Understanding Consumers.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>3:00 PM-3:20 PM</td>
                <td>Realistic Human Generation with Controllable Poses Using 3D Priors<br>
                  <b>Authors</b>:Ruifeng Bai, Xiaohang Liu, Haozhe Jia, Wei Zhang, Changpeng Yang, Di Xu
                </td>
                <td><a href="./asset/CameraReady/Realistic Human Generation with Controllable Poses Using 3D Priors.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>3:20 PM-3:40 PM</td>
                <td>Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models<br>
                <b>Authors</b>:Gurusha Juneja, Sukrit Kumar</td>
                <td><a href="./asset/CameraReady/Prompt Propose.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr>
                <td>3:40 PM-4:00 PM</td>
                <td>Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images<br>
                <b>Authors</b>:Snehal Singh Tomar, A.N. Rajagopalan</td>
                <td><a href="./asset/CameraReady/La2Sem.pdf" class="pdf-link" target="_blank">[PDF]</a></td>
              </tr>
              <tr class="gray_background">
                <td colspan="3"><b>4:00 PM-5:00 PM——Poster Session</b></td>
              </tr>
            </table>
            <b>Host:</b> Benjamin MacAdam<br>
            <b>Date: </b>Monday,Feb 26,2024
        </div>
      </div>
    </div>
  </section>


    <!--==========================================
  =            Challenge Section            =
  ===========================================-->
  
  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12"> 
          <div class="section-title">
            <h2 id="challenge">Digital Human Challenge</h2>
          </div>
          <!-- Deep learning has achieved significant success in multimedia fields, however research in adversarial learning also shows that it is highly vulnerable to adversarial examples. We invite submissions on any aspect of adversarial machine learning in multimedia deep learning systems. -->
          The challenges' deadline is due at <b>January 1st, 2024</b>.<br>
          <b><i>Note that teams that win the prize should submit their code and a brief summary at the end of the competition.</i></b><br>
          <strong>Task 1:Self-Supervised Face Geometry Reconstruction Competition</strong>
          <ul style="margin-left: 40px">
          <li>The objective of this task is to introduce self-supervised learning for face appearance reconstruction.
            Traditional photometric-based methods require capturing multi-view images under different lighting
            conditions to obtain high-quality facial appearance assets. However, this often requires the use of
            large devices like light-stages. In this competition, we encourage participants to calculate detailed
            digital face appearances from limited lighting and a small number of photos without relying on
            high-end equipment. Participants need to design a self-supervised learning framework to decompose
            detailed normal maps and a displacement map.</li>
          <li><b>Link:</b>   <a href="https://www.codabench.org/competitions/1601/" style="color:blue;">https://www.codabench.org/competitions/1601/</a></li>
          <li><b>Organizers:</b>   Yuhao Cheng, Xingyu Ren, Xuanchen Li (Shanghai Jiao Tong University), Huang Xu (Huawei Cloud) </li>
          <li><b>Contact:</b>   chengyuhao@sjtu.edu.cn</li>
          </ul>
          <table class="rank-table">
            <tr>
              <th>Ranking</th>
              <th>Team</th>
              <th>Institution</th>
            </tr>
            <tr>
              <td>1</td>
              <td>iFLYTEK-CV</td>
              <td>iFLYTEK Research & University of Science and Technology of China</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Excit AI</td>
              <td>Shanghai Excit AI Technology</td>
            </tr>
            <tr>
              <td>3</td>
              <td>USTC-IAT-United
              </td>
              <td>University of Science and Technology of China & Unisound AI Technology Co.,Ltd
              </td>
            </tr>
          </table>
          <strong>Task 2: Semi-supervised 3D Skull Reconstruction</strong>
          <ul style="margin-left: 40px">
          <li>
            In this task, we expect the cranial reconstruction algorithm to reconstruct the anatomical geometry
            of the skull based on the information provided by MRI, including but not limited to the precise
            geometry of the external surface of the face, the internal soft tissue structures of the face, and
            the characteristic points of the skull. This competition encourages participants to develop a semi-
            supervised learning-based cranial segmentation algorithm for MRI images using machine learning and
            deep learning techniques. The segmentation algorithm can fully exploit the information of unlabeled
            training samples based on a small number of labeled samples while improving the performance and
            generalization of the segmentation model. Ultimately, the high-quality segmentation results obtained
            on MRI will serve as the basis for 3D skull reconstruction.</li>
          <li><b>Link:</b>   <a href="https://www.kaggle.com/competitions/skull-reconstruction" style="color:blue;">https://www.kaggle.com/competitions/skull-reconstruction</a></li>
          <li><b>Organizers:</b>   Hengfei Cui, Yong Xia, Fan Zheng, Yifan Wang (Northwestern Polytechnical University) </li>
          <li><b>Contact:</b>   zhengfan@mail.nwpu.edu.cn</li>
          </ul>
          <table class="rank-table">
            <tr>
              <th>Ranking</th>
              <th>Team</th>
              <th>Institution</th>
            </tr>
            <tr>
              <td>1</td>
              <td>PPW</td>
              <td>Northwestern Polytechnical University & Southwest Jiaotong University & Hubei University of Technology
              </td>
            </tr>
            <tr>
              <td>2</td>
              <td>USTC_IAT_United
              </td>
              <td>University of Science and Technology of China
              </td>
            </tr>
            <tr>
              <td>3</td>
              <td>zhanggod

              </td>
              <td>Huazhong University of Science and Technology
              </td>
            </tr>
          </table>
          <strong>Task 3: Multi-modal Learning for Audio-driven Talking Head Generation</strong>
          <ul style="margin-left: 40px">
          <li>In this task, we specifically focus on the ability to generate talking heads with realistic facial
            expressions and natural head poses that match the accompanying audio. By learning from both the
            audio and visual data, this task focuses on developing a multi-modal learning model for talking head
            generation. We further design the following two settings to motivate the participants to tackle this
            task with 2D and 3D methods, respectively.<br>
            Single image setting: Participants are allowed to train their model with external data, and we will
            provide a single image for their models to animate.<br>
            Video setting: Participants are not allowed to train their model with external data, while a two-minute
            training video is provided for training a personalized talking head model.<br>
            The final output of both settings should be a talking head model that can be driven by any input audio.
            For the image setting, the synthesized talking head videos are expected to be lip-synchronized and
            with high fidelity. As for the video setting, the speakers should additionally have various natural
            poses during talking while preserving their identity.</li>
          <li><b>Link:</b>   <a href="https://www.kaggle.com/competitions/audio-driven-talking-head-generation/" style="color:blue;">https://www.kaggle.com/competitions/audio-driven-talking-head-generation/</a></li>
          <li><b>Organizers:</b>   Jingnan Gao(Shanghai Jiao Tong University) , Changpeng Yang, Yuan Gao, Li Li (Huawei Cloud) </li>
          <li><b>Contact:</b>   gjn0310@sjtu.edu.cn</li>
          </ul>
          <table class="rank-table">
            <tr>
              <th>Ranking</th>
              <th>Team</th>
              <th>Institution</th>
            </tr>
            <tr>
              <td>1</td>
              <td>BDIV Lab</td>
              <td>Xidian University
              </td>
            </tr>
            <tr>
              <td>2</td>
              <td>USTC-IAT-United
              </td>
              <td>University of Science and Technology of China,Ping An Technology
              </td>
            </tr>
            <tr>
              <td>3</td>
              <td>Excit AI
              </td>
              <td>Shanghai Excit AI Technology
              </td>
            </tr>
          </table>
          <strong>Task 4: Audio-Driven Co-Speech Gesture Video Generation</strong>
          <ul style="margin-left: 40px">
          <li>
            In this task, participants are required to synthesize co-speech gesture videos of a target person based on any given audio. 
            The final output should be a rendered video, instead of motion sequences. 
            Generally, the synthesized gesture motions in this task are expected to be natural, difficult to distinguish from captured videos, 
            and consistent with audio in terms of rhythm, semantics, and style. 
            We encourage participants to propose novel ideas for synthesizing high-fidelity co-speech gestures.</li>
          <li><b>Link:</b>   <a href="https://www.kaggle.com/competitions/audio-driven-co-speech-gesture-video-generation" style="color:blue;">https://www.kaggle.com/competitions/audio-driven-co-speech-gesture-video-generation</a></li>
          <li><b>Organizers:</b>   Minglei Li, Haoqi Zeng(Huawei Cloud),Zhensong Zhang, Xiaofei Wu, Yiren Zhou (Huawei Noah’s Ark Lab) </li>
          <li><b>Contact:</b>   zenghaoqi@huawei.com</li>
          </ul>
          <table class="rank-table">
            <tr>
              <th>Ranking</th>
              <th>Team</th>
              <th>Institution</th>
            </tr>
            <tr>
              <td>1</td>
              <td>HaiweiXue_DragonBooOOM
              </td>
              <td>Tsinghua University
              </td>
            </tr>
            <tr>
              <td>2</td>
              <td>USTC_IAT_United
              </td>
              <td>University of Science and Technology of China
              </td>
            </tr>
            <tr>
              <td>3</td>
              <td>XDU_VIPSLab
              </td>
              <td>Xidian University
              </td>
            </tr>
          </table>

          <!-- <strong>Challenge Site</strong>: <a href="https://practical-dl.sensecore.cn/" style="color:blue;">https://practical-dl.sensecore.cn</a> 
          <br>
          <strong>Submission Due</strong>: 31th Dec, 2022 (AoE) -->
        </div>
      </div>
    </div>
  </section>

<!--==========================================
=            Organizers Section            =
===========================================-->

<section class="popular-deals section bg-gray"> 
  <!-- Container Start -->
  <div class="container">
    <div class="row">
      <div class="col-12"> 
        <!-- Section title -->
        <div class="section-title">
          <h2 id="organizers">Organizer</h2>
        </div>

        <div class="card-deck"> 

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/yanyichao.jpeg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://daodaofr.github.io/">Yichao Yan</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Shanghai Jiao Tong University<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/dixu.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://ieeexplore.ieee.org/author/37086190386">Di Xu</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud Computing Technologies Co., Ltd<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/haozhejia.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://scholar.google.com.au/citations?user=8YMYwMwAAAAJ&hl=en9">Haozhe Jia</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p><br>Huawei Cloud Computing Technologies Co., Ltd<br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
  </div>
  <br>
    <div class="card-deck"> 
        <div class="card">
          <div class="category-block">
            <table width="80%" border="0" align="center">
              <tbody align="center">
                <tr height="200px">
                  <td><img src="./asset/matthias.jpeg" alt="" class="rounded-circle" width="60%"></td>
                </tr>
                <tr height="30px">
                  <td><h5 class="text-center"><a href="https://niessnerlab.org/">Matthias Nießner</a></h5></td>
                </tr>
                <tr height="60px">
                  <td><p><br>Technical University of Munich
                    <br><br><br><br></p></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

        <div class="card">
          <div class="category-block">
            <table width="80%" border="0" align="center">
              <tbody align="center">
                <tr height="200px">
                  <td><img src="./asset/dengjiankang.png" alt="" class="rounded-circle" width="60%"></td>
                </tr>
                <tr height="30px">
                  <td><h5 class="text-center"><a href="https://jiankangdeng.github.io/">Jiankang Deng</a></h5></td>
                </tr>
                <tr height="60px">
                  <td><p><br> Imperial College London，The department of
                    computing<br><br><br></p></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

          <div class="card">
            <div class="category-block">
              <table width="80%" border="0" align="center">
                <tbody align="center">
                  <tr height="200px">
                    <td><img src="./asset/kangkangyin.jpg" alt="" class="rounded-circle" width="60%"></td>
                  </tr>
                  <tr height="30px">
                    <td><h5 class="text-center"><a href="https://www.cs.sfu.ca/~kkyin/">KangKang Yin</a></h5></td>
                  </tr>
                  <tr height="60px">
                    <td><p>Simon Fraser University, School of Computing Science<br><br><br><br></p></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
                   
  </div>

      </div>
    </div>
  </div>
  <!-- Container End --> 
  </section>



    <!--==========================================
  =            Paper Submission Section            =
  ===========================================-->
  
  <!--  <section class="section"> 
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="section-title">
            <h2 id="paper-submission">Paper Submission</h2>
          </div>
          <strong>Format</strong>: Submissions need to be anonymized and follow AAAI 2022 author instructions. The workshop considers two types of submissions: (1) Long Paper: Papers are limited to <strong>7</strong> pages excluding references; (2) Extended Abstract: Papers are limited to <strong>4</strong> pages including references.

    <br>
    <strong>Submission Server</strong>: <a href="https://cmt3.research.microsoft.com/">https://cmt3.research.microsoft.com/</a> .
    
  </div>
  </div>
  
-->
    <!-- br>
    <br> -->
    
  <!--============================
  =            Footer            =
  =============================-->
  
  <!-- Footer Bottom -->
  <footer class="footer-bottom"> 
    <!-- Container Start -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12 col-12"> 
          <!-- Copyright -->
          <div class="copyright">
            <p class="text-center">For any further questions, you can contact <a href="mailto:yanyichao@sjtu.edu.cn"><font color="#5672f9">Yichao Yan</font></a>.</p>
            <!-- <p class="text-center">For any further questions, you can contact <a href="mailto:qinhaotong@buaa.edu.cn"><font color="#5672f9">Haotong Qin</font></a>, <a href="mailto:xinyun.chen@berkeley.edu"><font color="#5672f9">Xinyun Chen</font></a>, or <a href="mailto:yingwei.li@jhu.edu"><font color="#5672f9">Yingwei Li</font></a>.</p> -->
              <br class="clear">
          </div>
        </div>
      </div>
    </div>
  </footer>
  
  <!-- JAVASCRIPTS --> 
  <script src="./res/jquery.min.js"></script> 
  <script src="./res/jquery-ui.min.js"></script> 
  <script src="./res/tether.min.js"></script> 
  <script src="./res/jquery.raty-fa.js"></script> 
  <script src="./res/popper.min.js"></script> 
  <script src="./res/bootstrap.min.js"></script> 
  <script src="./res/bootstrap-slider.min.js"></script> 
  <script src="./res/slick.min.js"></script> 
  <script src="./res/jquery.nice-select.min.js"></script> 
  <script src="./res/jquery.fancybox.pack.js"></script> 
  <script src="./res/SmoothScroll.min.js"></script> 
  <script src="./res/scripts.js"></script>
  
  
  
</body></html>
